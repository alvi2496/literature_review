% !TEX root = main.tex
\section{Identification}
\label{sect:identification}
Automated identification of the design point based on the previous labelled data is the main challenge towards the envisioned tool of any recommendation or auto correction tool. To achieve this, the following steps are necessary to address: \begin{itemize}
	\item Which projects are selected?
	\item How are the annotation done?
	\item How is the data cleared and preprocessed?
	\item Which feature is considered?
	\item Which machine learning algorithms are used?
	\item How are the hyper parameter tuned? \end{itemize}
The rest of the section is dedicated to discuss the procedures that are followed and explained to reach certain result in different literature. Also the findings are discussed and the contribution of each work is outlined as well as some potential limitations and future work. 
 
The identification process and accuracy of trained individual can often differ from collaborative environment. Though everybody in the review team is familiar \cite{Sousa2017} with the system, the identification can be different depending on factors as small as preference. Brunet et al.,\cite{Brunet2014a} has claimed to be the first to discuss about the importance of identification of the design discussions in code reviews. The whole set of the comments on pull requests, commits, or issues are considered to be the a discussion and then classified the discussion as design if the whole set of discussion contain at least one comment that is related to the design of the software system. For the creation of initial dataset, two of the authors of the paper classified the identical set of 1000 discussions individually and did not set any specific rule to label those discussions to avoid bias. After that, they have taken the 967 discussions that matched with each other and used them to train Naive Bayes and Decision Tree and evaluated them with 10 fold cross validation. Their study shows that, Decision tree outperforms Naive Bayes with a accuracy of 94 {$\pm$} 1\%. Though this study demonstrate a very simple yet effective way of identifying design discussions, we can not get more specific information about the design topics as this study did not organize the design discussions in categories. 

In 2016, Shakiba \cite{Shakiba2016} revisited this work by trying to answer the same question by taking a different approach and extended the work in multiple areas that includes: obtaining meta-data from various platforms that hosts these kinds of discussions (SourceForge and GitHub) and providing with a comprehensive evaluation of different machine learning techniques. They used a domain specific language named Boa, which is used for mining large scale software repositories and randomly selected five projects in different software repository each containing 400 random comments resulting in a sample of 2000 comments. They have used WEKA to evaluate the models and found out that, Support Vector Machine and Random Forrest performs neck on neck but Random Forest has a slight edge with a superior G-Mean of 75.01. The kind of feature vector used to extract the features of the commits was not mentioned in the study which has a very significant part to play in the accuracy of any machine learning model. But this study extends the Brunet work by providing us with a comprehensive performance evaluation of the machine learning models to identify the design topics from the discussions.

The above mentioned studies provided us with a preliminary steps of identifying design points but, the they do not organize the discussion to make any meaningful categorization of the design topics that can provide more insight on understanding how design information can provide an opportunity to build some kind of tools that can assist developer understanding the design of a specific software system. Viviani et al.,\cite{Viviani2018} studied the different categories of the discussions and presented us with a categorized and organized view of the design discussions. For this task, they extracted the comment individually rather that taking the whole set of discussions in the comment thread. They argued that taking the whole set of discussion can limit the possibility of getting the comments that contain design points because a comment thread can have other point along with some design points. They have marked these comments as a paragraph and attempted to classify them based on methods like TextRank \cite{Mihalcea2004}. For the coding of the discussions, they used two coders and let the coders to manually label the paragraphs with a specific set or rules that has been predefined by the authors of the paper. In the first trial, 50 paragraphs were coded and the trial was repeated for three times. After the coding of the paragraph, Cohen's Kappa Coefficient was used to measure the inter coder agreement which was 0.64 which can be considered as ``substantial agreement''. This study provides an in depth analysis of the methods that can be used to identify the design topics but, there is no information on the approach, difficulty and feasibility of the technical implementation. Although nothing is studied to simulate the practicality of an automatic identification, this study on identifying design point can be very useful to organize and categorize different design points.

Self admitted technical debt through source code comments can also be good place to look for developer written remark on a piece of code. The study of identifying these comments as technical debts or design issues can also have a significant role towards identifying design discussions. Maldonado et al. \cite{Maldonado2017} used Natural Language Processing to identify self admitted technical debt that are present through source code comments. They have extracted comments from ten open source project and then applied five filtering heuristics to remove some irrelevant comments such as license comments. To reduce the risk of generating biased datasets, multiple labellers were used and they were given an one hour tutorial before doing so. They used Stanford classifier which is basically a Java implementation of maximum entropy classifier and trained the classifier with the comments of 9 projects out of the 10 projects. Then the classifier was tested on the 1 project that was left out during the training phase. From the classification the found out that, the two most common types of self-admitted technical debt are design and requirement debt illustrating defect, test, and documentation debt together represent less that 10 percent of all self- admitted technical debt comments. They also defined some domain specific stopwords to avoid repeated features. They implemented F1-Measure as their performance metric which can also a very effective take away from their study. They researched on code comments which are easier and less error prone compared to the code comments. Also, the length of the code comments are fairly limited where code comments can be very lengthy which can contribute to the degradation of the performance of Natural Language Processing. However, this study can be implemented to great extend with some intelligent data preprocessing to achieve great results on automatically identifying design discussions.

Since Software Issue Discussion threads are also a place where developers express their concerns through comments, the issue tracker discussions can considered similar to code comments. The studies on identifying patterns of issue discussions can be a very effective idea to integrate in the identification of the design discussions. A supervised investigation in \cite{Arya2019} indicates that the classification result defers based on prior knowledge and new data as well as classification model and feature type. After manually classifying issues from three most popular machine learning libraries, they found out some main information types in issues discussion which can very important to different stakeholders. They have mentioned the sentence preprocessing to be a very crucial part of the process and taken extensive measures to achieve that. For the vectorization of the sentences, they have employed TF-IDF  and conversational feature extraction. For the adjustment and the balance of the dataset, oversampling and weight adjustment is implemented. They have performed the verification on 4330 annotated sentences and found that, Random Forest can effectively detect most sentence types using conversational features such where Logistic Regression can yield satisfactory performance using textual features for certain information types. They have also used some hyper parameter tuning to optimally tune the machine learning model to that specific problem. This study touches almost all the standard procedures to achieve state-of-the-art text classification and can be extended further to clarify some more steps such as data collection or cleaning process which are considered to be vital step of text classification.

All the above mentioned works tried to find some pipeline that is best suited for their particular requirement. All the work conducted standard manual labelling procedure as well as validation of their work. Although each study of different pipelines has some significant contribution in achieving our envisioned tool, the study of the classifiers being accurate at a certain level for all kinds of data is very limited. However, these studies can be taken further and combined together to reach our goal of accurately identifying design points in discussions.                         