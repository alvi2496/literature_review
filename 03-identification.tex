% !TEX root = main.tex
\section{Identification}
\label{sect:identification}

Before looking into the developer discussion to explore some design concept of the project, we need to first understand how do the software developers understand and identify the design points. A study by Sousa et al.,\cite{Sousa2017} revealed six preeminent strategies named as: smell-based, problem-based, principle- based, element-based, quality attribute-based and pattern- based that are adopted by developers to identify the design issues. After the strategies are revealed, they experimented with familiarity and unfamiliarity scenario to qualitatively investigate the performance of their strategy. Without any surprise, familiarity with the system scored higher true positive scores than unfamiliarity. After quantifying those strategy, the count the occurrence of strategy based on the usage to find out the strategy that have the higher percentage of success. The six strategies were fixed for all the subjects which made the subjects to identify the design issues based on only those six strategies and no information on the accuracy of the hybrid strategies is not explained. Mixing up those strategies with each other to implement a hybrid strategy can yield some very interesting results. Also the order of using those strategies can affect the performance as well. However, their strategies can be used to identify design issue in code review discussions.
     
While it is very important to identify the design issues while the software is in the development phase, it is not practically feasible all the time to do that due to the extreme focus on achieving features and functionality within a specific amount of time. It is very important to automate the identification process of the design issues. Sousa in a more practical approach \cite{Sousa2018} has provided with a grounded theory on identifying design issues in source code level. For the experiment, projects are chosen with four specific criteria and developer are selected and categorized depending on their experience. The experiment has been divided in four activities: subjects characterization, training, problem identification and follow-up and six specific symptoms were investigated. Then the helpfulness of the system according to the developers is measured based on the count of the symptoms while it was applied and the number of contributions. Although some multi-trial industrial experiment to investigate the validation of their theory, some execution of empirical to access some more knowledge of the theory's proposition to successful detection.

The identification process and accuracy of trained individual can often differ from collaborative environment. Though everybody in the review team is familiar \cite{Sousa2017} with the system, the identification can be different depending on factors as small as preference. Brunet et al.,\cite{Brunet2014a} has claimed to be the first to discuss about the importance of identification of the design discussions in code reviews. The whole set of the comments on pull requests, commits, or issues are considered to be the a discussion and then classified the discussion as design if the whole set of discussion contain at least one comment that is related to the design of the software system. For the creation of initial dataset, two of the authors of the paper classified the identical set of 1000 discussions individually and did not set any specific rule to label those discussions to avoid bias. After that, they have taken the 967 discussions that matched with each other and used them to train Naive Bayes and Decision Tree and evaluated them with 10 fold cross validation. Their study shows that, Decision tree outperforms Naive Bayes with a accuracy of 94 ± 1\%. Though this study demonstrate a very simple yet effective way of identifying design discussions, we can not get more specific information about the design topics as this study did not organize the design discussions in categories. 

In 2016, Shakiba \cite{Shakiba2016} revisited this work by trying to answer the same question by taking a different approach and extended the work in multiple areas that includes: obtaining meta-data from various platforms that hosts these kinds of discussions (SourceForge and GitHub) and providing with a comprehensive evaluation of different machine learning techniques. They used a domain specific language named Boa, which is used for mining large scale software repositories and randomly selected five projects in different software repository each containing 400 random comments resulting in a sample of 2000 commits. They have used WEKA to evaluate the models and found out that, Support Vector Machine and Random Forrest performs neck on neck but Random Forest has a slight edge with a superior G-Mean of 75.01. The kind of feature vector used to extract the features of the commits was not mentioned in the study which has a very significant part to play in the accuracy of any machine learning model. But this study extends the Brunet work by providing us with a comprehensive performance evaluation of the machine learning models to identify the design topics from the discussions.

The above mentioned studies provided us with a preliminary steps of identifying design points but, the they do not organize the discussion to make any meaningful categorization of the design topics that can provide more insight on understanding how design information is can provide an opportunity to build some kind of tools that can assist developer understand the design of a specific software system. Viviani et al.,\cite{Viviani2018} studied the different categories of the discussions and presented us with a categorized and organized view of the design discussions. For this task, they extracted the comment individually rather that taking the whole set of discussions in the comment thread. They argued that, taking the whole set of discussion can limit the possibility of getting the comment that contain design points because a comment thread can have other point along with some design points. They have marked these comments as a paragraph and attempted to classify them based on methods like TextRank \cite{Mihalcea2004}. For the coding of the discussions, they used two codes and let the coder to manually label the paragraphs with a specific set or rules that has been predefined by the authors of the paper. In the first trial, 50 paragraphs were coded and the trial was repeated for three times. After the coding of the paragraph, Cohen’s Kappa Coefficient was used to measure the inter rater agreement which was 0.64 which can be considered as ``substantial agreement''. This study provides an in depth analysis of the methods that can be used to identify the design topics but, there is no information on the approach, difficulty and feasibility of the technical implementation. Although nothing is studied to simulate the practicality of an automatic identification, this study on identifying design point can be very useful to organize and categorize different design points.           